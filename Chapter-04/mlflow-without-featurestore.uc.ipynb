{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d0aac4c-58bc-4145-b9b7-f20c5512cab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLflow introduction.\n",
    "\n",
    "This tutorial covers an example of how to use the integrated MLflow tracking capabilities to track your model training with the integrated feature store.\n",
    "  - Import data from the Delta table that contains feature engineered datasets.\n",
    "  - Create a baseline model for churn prediction and store it in the integrated MLflow tracking server. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06cf4ebd-a61c-4762-884e-534434a4f37d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###0. SETUP -- Databricks Spark cluster:  \n",
    "\n",
    "1. **Create** a cluster by...  \n",
    "  - Click the `Compute` icon on the left sidebar and then `Create Cluster.` \n",
    "  - In `Policy` select `Unrestricted`.\n",
    "  - Enter any text, i.e `demo` into the cluster name text box.\n",
    "  - Select `Single Node` in the cluster mode.\n",
    "  - Select the `Databricks runtime version` value `13.3 LTS (Scala 2.12, Spark 3.4.1)` from the `ML` tab.\n",
    "  - On `AWS`, select `i3.xlarge` / on `Azure`, select `Standard_DS4_V2` as __Node type__.\n",
    "  - Click the `create cluster` button and wait for your cluster to be provisioned\n",
    "3. **Attach** this notebook to your cluster by...   \n",
    "  - Click on your cluster name in menu `Detached` at the top left of this workbook to attach it to this workbook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0065ebb-69a4-40e6-91a6-beb369537b3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#install latest version of sklearn\n",
    "%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a682635-9d30-4947-9816-5ea7affe26c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1) Importing the desired libraries and defining few constants.\n",
    "\n",
    "- Note:<br>\n",
    "  - In this example the feature table is the same as we created in Chapter 3, however we will not use the featurestore API to access the data in the feature table.<br>\n",
    "  - As explained in chapter 3, all the offline feature tables are backed as Delta tables and are searchable through the integrated Hive metastore in Databricks. This allows us to read these tables like a regular external or managed table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51e33141-7e2e-49d8-a01d-7ec2774487ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "from databricks.feature_store import FeatureLookup\n",
    "import typing\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a125f8e-99b2-4bb3-92e3-500a6d313bc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# NF = No Features\n",
    "\n",
    "#The email you use to authenticate in the Databricks workspace\n",
    "USER_EMAIL = \"juan.lamadrid@databricks.com\"\n",
    "#Location where the MLflow experiement will be listed in user workspace\n",
    "EXPERIMENT_NAME = f\"/Users/{USER_EMAIL}/Bank_Customer_Churn_Analysis_NF\"\n",
    "#Name of the model\n",
    "MODEL_NAME = \"juan_dev.mldbxbook.random_forest_classifier_featurestore_NF\"\n",
    "#This is the name for the entry in model registry\n",
    "MODEL_REGISTRY_NAME = \"juan_dev.mldbxbook.Bank_Customer_Churn_NF\"\n",
    "# we have all the features backed into a Delta table so we will read directly\n",
    "FEATURE_TABLE = \"juan_dev.mldbxbook.bank_customer_features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "477311b8-890b-489e-8f2e-880fc0d026fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2) Build a simplistic model that uses the feature store table as its source for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "813be4df-d29c-4d37-9182-567eeac4c7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "  # Now we will read the data directly from the feature table\n",
    "display(spark.table(FEATURE_TABLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888fc931-317c-4914-8c35-82df97429184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# set experiment name\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run():  \n",
    "  TEST_SIZE = 0.20\n",
    "  \n",
    "  # Now we will read the data directly from the feature table\n",
    "  training_df = spark.table(FEATURE_TABLE)\n",
    "  \n",
    "  # convert the dataset to pandas so that we can fit sklearn RandomForestClassifier on it\n",
    "  train_df = training_df.toPandas()\n",
    "  \n",
    "  # The train_df represents the input dataframe that has all the feature columns along with the new raw input in the form of training_df.\n",
    "  X = train_df.drop(['Exited'], axis=1)\n",
    "  y = train_df['Exited']\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=54, stratify=y)\n",
    "  \n",
    "  # here we will are not doing any hyperparameter tuning however, in future we will see how to perform hyperparameter tuning in scalable manner on Databricks.\n",
    "  model = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "  signature = mlflow.models.signature.infer_signature(X_train, model.predict(X_train))\n",
    "  \n",
    "  predictions = model.predict(X_test)\n",
    "  fpr, tpr, _ = metrics.roc_curve(y_test, predictions, pos_label=1)\n",
    "  auc = metrics.auc(fpr, tpr)\n",
    "  accuracy = metrics.accuracy_score(y_test, predictions)\n",
    " \n",
    "  # get the calculated feature importances.\n",
    "  importances = dict(zip(model.feature_names_in_, model.feature_importances_))  \n",
    "  # log artifact\n",
    "  mlflow.log_dict(importances, \"feature_importances.json\")\n",
    "  # log metrics\n",
    "  mlflow.log_metric(\"auc\", auc)\n",
    "  mlflow.log_metric(\"accuracy\", accuracy)\n",
    "  # log parameters\n",
    "  mlflow.log_param(\"split_size\", TEST_SIZE)\n",
    "  mlflow.log_params(model.get_params())\n",
    "  # set tag\n",
    "  mlflow.set_tag(MODEL_NAME, \"mlflow demo\")\n",
    "  # log the model itself in mlflow tracking server\n",
    "  mlflow.sklearn.log_model(\n",
    "    model, \n",
    "    MODEL_NAME, \n",
    "    signature=signature, \n",
    "    input_example=X_train.iloc[:4, :]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bf77187-fbb9-451f-abb0-d33c42c70c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "#initialize the mlflow client\n",
    "client = MlflowClient()\n",
    "#get the experiment id \n",
    "experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "#get the latest run id which will allow us to directly access the metrics, and attributes and all th einfo\n",
    "run_id = mlflow.search_runs(experiment_id, order_by=[\"start_time DESC\"]).head(1)[\"run_id\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10b0121a-6a82-4d96-9ef7-9f4c55eb7463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#now we will register the latest model into the model registry\n",
    "new_model_version = mlflow.register_model(f\"runs:/{run_id}/{MODEL_NAME}\", MODEL_REGISTRY_NAME)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mlflow-without-featurestore.uc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba670fa7-93cf-4558-9f37-58d5c6a06cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MLflow introduction.\n",
    "\n",
    "This tutorial covers an example of how to use the integrated MLflow tracking capabilities to track your model training with the integrated feature store.\n",
    "  - Import data that was previously registered in the feature store table.\n",
    "  - Create a baseline model for churn prediction and store it in the integrated MLflow tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26188a04-5f3c-42b7-8c34-b22c7dcfe2eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#install latest version of sklearn\n",
    "%pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5bfe60c-afb9-4156-aab3-0e72362d4f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1) Importing the desired libraries and defining few constants and creating training set from the registered feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe908da-e447-4e4d-9f97-f8e90c1eebdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "import typing\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd771f3-784b-4b49-85b9-32a2aaf076ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Name of the model\n",
    "MODEL_NAME = \"juan_dev.data_science.random_forest_classifier_featurestore\"\n",
    "#This is the name for the entry in model registry\n",
    "MODEL_REGISTRY_NAME = \"juan_dev.data_science.bank_customer_churn\"\n",
    "#The email you use to authenticate in the Databricks workspace\n",
    "USER_EMAIL = \"juan.lamadrid@databricks.com\"\n",
    "#Location where the MLflow experiement will be listed in user workspace\n",
    "EXPERIMENT_NAME = f\"/Users/{USER_EMAIL}/bank_customer_churn_analysis_experiment\"\n",
    "# we have all the features backed into a Delta table so we will read directly\n",
    "FEATURE_TABLE = \"juan_dev.data_science.bank_customer_features\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55311a84-ca1c-428d-81e1-de4db6d656fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# this code is just for demonstration and you can utilize this as starting point and build more errorhandling around it.\n",
    "class Feature_Lookup_Input_Tuple(typing.NamedTuple):\n",
    "  fature_table_name: str\n",
    "  feature_list: typing.Union[typing.List[str], None] \n",
    "  lookup_key: typing.List[str]\n",
    "\n",
    "# this code is going to generate feature look up based on on the list of feature mappings provided.\n",
    "def generate_feature_lookup(feature_mapping: typing.List[Feature_Lookup_Input_Tuple]) -> typing.List[FeatureLookup]:  \n",
    "  lookups = []\n",
    "  for fature_table_name, feature_list, lookup_key in feature_mapping:\n",
    "    lookups.append(\n",
    "          FeatureLookup(\n",
    "          table_name = fature_table_name,\n",
    "          feature_names = feature_list,\n",
    "          lookup_key = lookup_key \n",
    "      )\n",
    "    )\n",
    "  return lookups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "badbf4ef-b5b2-41cc-843b-83a6865c4c86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2) Build a simplistic model that uses the feature store table as its source for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e792dcaa-3a4e-471e-8501-134d8cbeb8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c265eab-29ac-4ae9-bc49-e84ab2753aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineeringClient()\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "with mlflow.start_run():  \n",
    "  TEST_SIZE = 0.20\n",
    "  \n",
    "  #define the list of features we want to get from feature table\n",
    "  #If we havse to combine data from multiple feature tables then we can provide multiple mappings for feature tables \n",
    "  features = [Feature_Lookup_Input_Tuple(FEATURE_TABLE,[\"CreditScore\" , \"Age\", \"Tenure\",\\\n",
    "              \"Balance\", \"NumOfProducts\", \"HasCrCard\",\\\n",
    "              \"IsActiveMember\", \"EstimatedSalary\", \"Geography_Germany\",\\\n",
    "              \"Geography_Spain\", \"Gender_Male\"], [\"CustomerId\"] )]\n",
    "\n",
    "  lookups = generate_feature_lookup(features)\n",
    "  \n",
    "  #Now we will simulate receiving only ID's of customers and the label as input at the  time of inference\n",
    "  training_df = spark.table(FEATURE_TABLE).select(\"CustomerId\", \"Exited\")\n",
    "  \n",
    "  #Using the training set we will combine the training dataframe with the features stored in the feature tables.\n",
    "  training_data = fe.create_training_set(\n",
    "    df=training_df,\n",
    "    feature_lookups=lookups,\n",
    "    label=\"Exited\",\n",
    "    exclude_columns=['CustomerId']\n",
    "  )\n",
    "  \n",
    "  #convert the dataset to pandas so that we can fit sklearn RandomForestClassifier on it\n",
    "  train_df = training_data.load_df().toPandas()\n",
    "  \n",
    "  #The train_df represents the input dataframe that has all the feature columns along with the new raw input in the form of training_df.\n",
    "  X = train_df.drop(['Exited'], axis=1)\n",
    "  y = train_df['Exited']\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=54, stratify=y)\n",
    "  \n",
    "  #here we will are not doing any hyperparameter tuning however, in future we will see how to perform hyperparameter tuning in scalable manner on Databricks.\n",
    "  model = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "  \n",
    "  signature = mlflow.models.signature.infer_signature(X_train, model.predict(X_train))\n",
    "  \n",
    "  predictions = model.predict(X_test)\n",
    "  fpr, tpr, _ = metrics.roc_curve(y_test, predictions, pos_label=1)\n",
    "  auc = metrics.auc(fpr, tpr)\n",
    "  accuracy = metrics.accuracy_score(y_test, predictions)\n",
    " \n",
    "  #get the calculated feature importances.\n",
    "  importances = dict(zip(model.feature_names_in_, model.feature_importances_))  \n",
    "  #log artifact\n",
    "  mlflow.log_dict(importances, \"feature_importances.json\")\n",
    "  #log metrics\n",
    "  mlflow.log_metric(\"auc\", auc)\n",
    "  mlflow.log_metric(\"accuracy\", accuracy)\n",
    "  #log parameters\n",
    "  mlflow.log_param(\"split_size\", TEST_SIZE)\n",
    "  mlflow.log_params(model.get_params())\n",
    "  #set tag\n",
    "  mlflow.set_tag(MODEL_NAME, \"mlflow and feature store demo\")\n",
    "  #log the model itself in mlflow tracking server\n",
    "  mlflow.sklearn.log_model(model, MODEL_NAME, signature=signature, input_example=X_train.iloc[:4, :])\n",
    "\n",
    "  # finally to make the feature store track what features are being used by our model we call log_model with the feature store client\n",
    "  fe.log_model(\n",
    "    model=model,\n",
    "    artifact_path=MODEL_NAME,\n",
    "    flavor=mlflow.sklearn,\n",
    "    training_set=training_data,\n",
    "    registered_model_name=MODEL_REGISTRY_NAME #\n",
    "  )\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "592d8870-4138-44b8-86ea-63016b2f823b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3) Now that we have the model logged to the MLflow tracking server, we can get the latest version from the experiment and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45715dd2-8672-4989-a1f5-527103d495fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking.client import MlflowClient\n",
    "\n",
    "#initialize the mlflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "#get the experiment id \n",
    "experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "#get the latest run id which will allow us to directly access the metrics, and attributes and all th einfo\n",
    "run_id = mlflow.search_runs(experiment_id, order_by=[\"start_time DESC\"]).head(1)[\"run_id\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "129127b8-eae3-46e4-b353-fa36ea937619",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- With the feature store registration associated with the MLflow model, we don't have to specify any data loading and processing to happen other than a point to the raw data that features will be calculated from. \n",
    "- We can do batch predictions simply by accessing the feature store instance, providing the run_id and the model's name (MODEL_NAME below) with the raw data specified as the second argument. \n",
    "- If we want to provide new values for certain feature that is already part of the feature table, just include it in the new dataframe that we want to perform the prediction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ea1fe03-afbe-4f04-9330-3cc56121da65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#at the time of infernce you can provide just the CustomerId. This is the key that will perform all the lookup for the features automatically.\n",
    "predictions = fe.score_batch(model_uri=f\"runs:/{run_id}/{MODEL_NAME}\", df=spark.table(FEATURE_TABLE).select(\"CustomerId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1db22a0-d0e3-4c24-b508-f232ed0cc533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "706d7887-af2f-42e8-b525-0d7453db17ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a535ac63-c93f-4887-808f-bc3c5803489b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Uncomment to lines below and execute for cleaning up.\n",
    "'''\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "#get all the information about the current experiment\n",
    "experiment_id = mlflow.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "\n",
    "#list all the runs that are part of this experiment and delete them\n",
    "runs = mlflow.list_run_infos(experiment_id=experiment_id)\n",
    "for run in runs:\n",
    "  mlflow.delete_run(run_id = run.run_id)\n",
    "\n",
    "#finally delete the experiment  \n",
    "mlflow.delete_experiment(experiment_id=experiment_id)  \n",
    "\n",
    "client = MlflowClient()\n",
    "#delete the model registered in the registry to clear the linkage in thefeature store\n",
    "client.delete_registered_model(name=MODEL_REGISTRY_NAME)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mlflow-with-featurestore.uc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

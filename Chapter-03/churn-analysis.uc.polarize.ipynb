{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95fa6a09-029f-459a-bd44-06195fecdf7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* [**Customer Churn**](https://en.wikipedia.org/wiki/Customer_attrition) also known as Customer attrition, customer turnover, or customer defection, is the loss of clients or customers and is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64efc6c5-1ae7-4194-9f4c-b27b596158e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- https://docs.databricks.com/en/machine-learning/feature-store/example-notebooks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21b2e4f2-1731-42ad-8ae4-6485600af370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install polars\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ef0540a-97bd-4101-a289-3a98bb11acf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read more about reading files from Databricks repos at https://docs.databricks.com/repos.html#access-files-in-a-repo-programmatically\n",
    "# import os\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bde688cd-3fb8-4b9d-80c3-575bb279e2d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.databricks.workspaceUrl\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(spark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.databricks.clusterUsageTags.clusterId\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.databricks.workspaceUrl\"))\n",
    "print(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "350975d8-f104-48fe-a381-2fbcc5a21955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bank_df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"/Volumes/juan_dev/mldbxbook/mldbxbook/churn.csv\")\n",
    "    # .csv(f\"file:{os.getcwd()}/data/churn.csv\")\n",
    ")\n",
    "\n",
    "display(bank_df)\n",
    "\n",
    "bank_df.select(\"Surname\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5fb4241-e270-41f3-ac84-26e1b599df63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DATABASE_NAME = \"mldbxbook\"\n",
    "CATALOG_NAME = \"juan_dev\"\n",
    "\n",
    "# Create a new catalog with:\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {DATABASE_NAME}\")\n",
    "spark.sql(f\"USE SCHEMA {DATABASE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a7b6c77-509d-4f0f-847f-07c0aecc7fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select current_database(), current_catalog()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d47e89f-1628-4c03-a8da-ef743ad82458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bank_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{DATABASE_NAME}.raw_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edc1d202-4506-4b82-acb6-d77278f51e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Defining a feature engineering function that will return a Spark dataframe with a unique primary key.\n",
    "In our case it is the `CustomerId`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c1530f3-83d2-4b84-9560-e0df5e42752d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The `bank_df` DataFrame is already pretty clean, but we do have some nominal features that we'll need to convert to numeric features for modeling.\n",
    "\n",
    "These features include:\n",
    "\n",
    "* **`Geography`**\n",
    "* **`Gender`**\n",
    "\n",
    "We will also be dropping few features which dont add additional value for our model:\n",
    "* **`RowNumber`**\n",
    "* **`Surname`**\n",
    "\n",
    "### Create `compute_features` Function\n",
    "\n",
    "A lot of data scientists are familiar with Pandas DataFrames, so we'll use the [pyspark.pandas](https://spark.apache.org/docs/3.2.0/api/python/user_guide/pandas_on_spark/) library to one-hot encode these categorical features.\n",
    "\n",
    "**Note:** we are creating a function to perform these computations. We'll use it to refer to this set of instructions when creating our feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89ec94b7-dcbd-4e63-9c6b-13f37e69cfce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def compute_features(spark_df):\n",
    "    # use polars instead of pandas on spark\n",
    "    pl_ohe_df = (\n",
    "        pl.from_pandas(spark_df.toPandas())\n",
    "        .drop([\"RowNumber\", \"Surname\"])\n",
    "        .to_dummies(columns=[\"Geography\", \"Gender\"], drop_first=True)\n",
    "    ).cast(\n",
    "        {\n",
    "            \"Gender_Male\": pl.Int32,\n",
    "            \"Geography_Germany\": pl.Int32,\n",
    "            \"Geography_Spain\": pl.Int32,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return pl_ohe_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2554e0a5-003f-40b3-b7a3-f2a8ffccd675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compute Features\n",
    "\n",
    "Next, we can use our featurization function `compute_features` to create create a DataFrame of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0dd017b-47e3-4f70-bf73-442d067a60ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bank_features_df = compute_features(bank_df)\n",
    "display(bank_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1e1896a-c2c4-4dee-9157-6747f7f95e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_ohe_df = spark.createDataFrame(bank_features_df.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21d84bec-8d96-4ae7-b8e7-cebb0b9af947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Our first step is to instantiate the feature store client using `FeatureStoreClient()`.\n",
    "from databricks.feature_engineering import FeatureEngineeringClient, FeatureLookup\n",
    "\n",
    "fs = FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee405b44-11e3-420a-8151-07868f4b4bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bank_feature_table = fs.create_table(\n",
    "    name=f\"juan_dev.{DATABASE_NAME}.bank_customer_features\",  # the name of the feature table\n",
    "    primary_keys=[\"CustomerId\"],  # primary key that will be used to perform joins\n",
    "    schema=features_ohe_df.schema,  # the schema of the Feature table\n",
    "    description=\"This customer level table contains one-hot encoded categorical and scaled numeric features to predict bank customer churn.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7f02b7-1b39-43ce-be42-7db27870d998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Populate the feature table using write_table.\n",
    "Now, we can write the records from **`bank_features_df`** to the feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2c1f1b0-33b6-4912-8601-88468d8bb8bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fs.write_table(\n",
    "    df=features_ohe_df,\n",
    "    name=f\"{CATALOG_NAME}.{DATABASE_NAME}.bank_customer_features\",\n",
    "    mode=\"merge\",\n",
    ")\n",
    "# instead of overwrite you can choose \"merge\" as an option if you want to update only certain records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96cbb4c3-c3db-4a36-9a7d-fcb746e36d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dd8de56-1cce-4c14-b76f-ab552638b1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop feature table. This will drop the underlying Delta table as well.\n",
    "\n",
    "fs.drop_table(\n",
    "  name=f\"{CATALOG_NAME}.{DATABASE_NAME}.bank_customer_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f6b1f70-45c3-4d91-abd4-9dfdeaf9354b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note: <b>In you decide to drop table from UI follow the follwing steps.</b>.\n",
    "\n",
    "Follow the following steps:\n",
    "- Go to [Feature Store](/#feature-store/feature-store)\n",
    "- Select the feature tables and select `delete` after clicking on 3 vertical dots icon.\n",
    "\n",
    "Deleting the feature tables in this way requires you to manually delete the published online tables and the underlying Delta table separately."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "churn-analysis.uc.polarize",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "practical-machine-learning-on-databricks-LmnwjhkF-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
